{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Assignment\n",
    "In this lecture you will learn to work with a larger dataset.\n",
    "\n",
    "You will be doing the following things:\n",
    "\n",
    "<br>1) Get a brief introduction to classification, a specific type of predictive analytics\n",
    "<br>2) Using data preprocessing tools (check for missing data, handle missing data, check the data types, encode columns).\n",
    "<br>3) Explore the data with descriptive statistical tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Classification is a specific type of machine learning problem where we take a lot of input data (referred to as features) of an observation (one row in your dataframe) in order to predict something about the observation. This something is a decired target (also referred to as label). In a nutshell, we throw this data into a machine learning algorithm (we will import a machine learning library), which in turn will look for patterns that describes the relationship between the input and output data. Based on this, the machine learning algorithm will return a generic model to us, which we can use for predictions in the future.\n",
    "\n",
    "<br> In this assignment we are going to predict if it will rain in Australia the next day, based on various characteristics (features) that can be measured the previous day. Having collected a lot of historical data over many days, we can find such a relationship. Ideally, if we develop a good model we can predict Australia's weather tomorrow prospectively.\n",
    "\n",
    "<br> The \"RainTomorrow\" column is the so-called label. The \"Date\" column is simply an index. The other columns are the so-called features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.0\n",
    "Import Pandas, import the CSV <b>'weather_aus.csv'</b> file and print the first couple of rows. Take a couple of minutes to look at the columns and their values in order to become one with the data.\n",
    "\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Pandas has a <b>read_csv</b> method\n",
    "    </p>\n",
    "</details>\n",
    "<details><summary>Hint 2</summary>\n",
    "    <p>\n",
    "        Pandas has a <b>head()</b> method to print the first rows\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.1\n",
    "Print the number of rows and columns. Additionally, find the mean and standard deviation of all the columns.\n",
    "\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Pandas has a method called describe()\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.2\n",
    "By now we can see a couple of rows have some values called \"NaN\". This means that there is no actual value (<b>nulls</b>, a very important topic in computer science); it was missing in the dataset before we imported it. Missing values are very common, but having a lot of them can decrease performance. Find the number of missing values in the dataframe.\n",
    "\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Pandas has a method called isnull(). This returns True or False based on whether the value was null.\n",
    "    </p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2</summary>\n",
    "    <p>\n",
    "        True and False values amounts to 1 and 0, respectively, when you try to sum them up.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.3\n",
    "Find the fraction of missing values in the dataframe\n",
    "\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        You can use isnull() on a DataFrame\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.4\n",
    "We can see that the <b>Sunshine</b> column has the highest fraction of nulls. In fact, all the values are missing. The machine learning algorithm will not be able to learn any patterns from this, whereas we may remove the column right away. Do that.\n",
    "\n",
    "<details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        DataFrame has a method called <b>drop.</b>. Axis must be = 1, as we are dropping columns not rows.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.5\n",
    "As explained earlier, we will use the features to predict the label (i.e. the columns to predict RainTomorrow). Often, as well as for this dataset, we can have some metadata in our dataset that does not have any explanatory power per se (it does't help us predict the label) but it may still be useful to us. The Date column is an example of this. Another example can be a customer ID. If we assume that our classification model can predict RainTomorrow for any day, it is not important to look for patterns between the individual date for an observation and the label. Although, the column may still be useful for us. Long story short, we do not need to transform the Date column, but we can use it as an index in the DataFrame to seperate it from the remaining features.\n",
    "\n",
    "<b>Set the Date column as an index!</b>\n",
    "\n",
    "<br> <details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Dataframe has a method called set_index()\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.6.1\n",
    "\n",
    "XGBoost is a very popular machine learning algorithm used for tabular data (the kind of data we are working with). A standard XGBoost algorithm cannot fit a prediction model with columns that are not numeric (e.g. WindGustDir). We shall now dive into this.\n",
    "\n",
    "<b>Check the data types of all features in the Dataframe</b>\n",
    "\n",
    "<br> <details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Dataframe has an attribute called dtypes.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.6.3\n",
    "\n",
    "All non-object types are not necessarily continuous; there can for instance be binary columns (e.g. RainToday).\n",
    "<br><br><b>Create a list containing strings with the columns that are of type object: Call the list \"cat_list\". \n",
    "Create another list for the non-objects: Call the list \"num_list\".</b>\n",
    "\n",
    "<br> <details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Your can either do it manually, make a loop or some other sneaky idea.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.6.4\n",
    "\n",
    "In order to encode the categorical values, we need to make an active decision on what to do with the null values. Handling missing data is a large topic in itself, but we will simply <i>impute</i> all the missing categorical values with the string <b>'missing'</b>. Fill the numeric values with <b>0</b>.\n",
    "\n",
    "<br><b>Impute the categorical columns with the string 'missing'</b>\n",
    "\n",
    "<br> <details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Dataframe has a method called fillna() for this purpose. Remember to specify the columns you intend to run this operation on.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.6.5\n",
    "\n",
    "Before we can hand the data to the machine learning algorithm, we shall transform categorical columns. For this, we can use an OrdinalEncoder provided by the library Scikit Learn (there are several encoders that will do the job). An ordinal encoder simply maps all the categories into numeric values.\n",
    "\n",
    "<br><b>Import the OrdinalEncoder from sklearn and apply it for the categorical features </b>\n",
    "\n",
    "Find the documentation for OrdinalEncoder [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)\n",
    "\n",
    "<br> <details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        The encoder has a method called fit_transform(). It takes a dataframe as an input, and will spit out the transformed dataframe. Remember to specify the columns you need to transform (you have a list for this).\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.7.1\n",
    "\n",
    "One of the most important topics in machine learning, is the training and test split. Luckily it is not very complex to understand. As we talked about earlier, we train our model on data that we already have in order to make some predictions on data where we miss the label. Again, we know whether it was raining yesterday but we cannot be certain of tomorrow; we can only predict it. As we can never be certain we at least want to know how accuracte we are in this prediction. To figure this out, we will set aside some of the data we already have, in order to test how well our model performs on this data. This is called the training and test split. While there is no golden rule, it is common to use 75% of the data for training and 25% of the data for testing.\n",
    "\n",
    "<br><b>First, split the dataframe into two: one that contains the label (RainTomorrow) and one that contains all the features</b>\n",
    "\n",
    "<br> <details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "        Dataframe has a drop() method to remove a column.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.7.2\n",
    "\n",
    "<br><b>Split the data into X_train, X_test, y_train, y_test. Use Scikit Learn's train_test_split function for this. 75% of the observations for training and 25% for test</b>\n",
    "\n",
    "Find the documentation for train_test_split [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "<br> <details><summary>Hint 1</summary>\n",
    "    <p>\n",
    "      Look at the documentation ;-)\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.8\n",
    "\n",
    "Le grand final: Build a prediction model with XGBoost. To do this, you must: <br><b>1) Import xgboost. <br>2) Initialise an object of class XGBClassifier. <br>3) Use the method fit() of the object you just created. Giv the object your two training dataframes (the dataframe for features and the dataframe for labels). The fit method will look for patterns in your data, and this may take a few minutes, depending on your hardware. <br>4) Call method predict() on the object, and give the method your dataframe with test features as a parameter. Assign this statement to a variable called 'predictions' (these are your predictions for the test set).</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.9.1\n",
    "\n",
    "Try to test how your model performed using the accuracy function from below (you may have to insert your own variable names)...... How can we tell whether the accuracy is good or bad? We may look a bit more into this in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.9.2\n",
    "\n",
    "With the code snippet below you can see the most informative features (the features that XGBoost used the most for its predictions). You may replace the object with your own object name. Ideally, we can get rid of the features that was never used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.get_booster().get_fscore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
